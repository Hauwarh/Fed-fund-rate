{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8980f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch FOMC statements and minutes\n",
    "\n",
    "def fetch_fomc_statements(start_year=2000):\n",
    "    base_url = 'https://www.federalreserve.gov'\n",
    "    statements_url = f'{base_url}/monetarypolicy/fomccalendars.htm'\n",
    "    response = requests.get(statements_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all links to FOMC statements and minutes\n",
    "    links = soup.find_all('a', href=True)\n",
    "    fomc_links = [link['href'] for link in links if 'monetarypolicy/fomcminutes' in link['href'] or 'monetarypolicy/fomcstatement' in link['href']]\n",
    "    \n",
    "    # Filter links by start year\n",
    "    fomc_links = [link for link in fomc_links if any(str(year) in link for year in range(start_year, 2024))]\n",
    "    \n",
    "    # Fetch and store the content\n",
    "    data = []\n",
    "    for link in fomc_links:\n",
    "        full_url = f'{base_url}{link}'\n",
    "        response = requests.get(full_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.get_text()\n",
    "        data.append({'url': full_url, 'content': content})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fetch FOMC statements and minutes from 2000 onwards\n",
    "fomc_data = fetch_fomc_statements(start_year=2000)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "fomc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f70ff0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.federalreserve.gov/monetarypolicy/...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.federalreserve.gov/monetarypolicy/...   \n",
       "1  https://www.federalreserve.gov/monetarypolicy/...   \n",
       "2  https://www.federalreserve.gov/monetarypolicy/...   \n",
       "3  https://www.federalreserve.gov/monetarypolicy/...   \n",
       "4  https://www.federalreserve.gov/monetarypolicy/...   \n",
       "\n",
       "                                             content  \n",
       "0  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...  \n",
       "1  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...  \n",
       "2  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...  \n",
       "3  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...  \n",
       "4  \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe Fe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch FOMC statements and minutes\n",
    "\n",
    "def fetch_fomc_statements(start_year=2000):\n",
    "    base_url = 'https://www.federalreserve.gov'\n",
    "    statements_url = f'{base_url}/monetarypolicy/fomccalendars.htm'\n",
    "    response = requests.get(statements_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all links to FOMC statements and minutes\n",
    "    links = soup.find_all('a', href=True)\n",
    "    fomc_links = [link['href'] for link in links if 'monetarypolicy/fomcminutes' in link['href'] or 'monetarypolicy/fomcstatement' in link['href']]\n",
    "    \n",
    "    # Filter links by start year\n",
    "    fomc_links = [link for link in fomc_links if any(str(year) in link for year in range(start_year, 2024))]\n",
    "    \n",
    "    # Fetch and store the content\n",
    "    data = []\n",
    "    for link in fomc_links:\n",
    "        full_url = f'{base_url}{link}'\n",
    "        response = requests.get(full_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.get_text()\n",
    "        data.append({'url': full_url, 'content': content})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fetch FOMC statements and minutes from 2000 onwards\n",
    "fomc_data = fetch_fomc_statements(start_year=2000)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "fomc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to fetch FOMC statements and minutes with dates\n",
    "\n",
    "def fetch_fomc_statements_with_dates(start_year=2000):\n",
    "    base_url = 'https://www.federalreserve.gov'\n",
    "    statements_url = f'{base_url}/monetarypolicy/fomccalendars.htm'\n",
    "    response = requests.get(statements_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all links to FOMC statements and minutes\n",
    "    links = soup.find_all('a', href=True)\n",
    "    fomc_links = [link['href'] for link in links if 'monetarypolicy/fomcminutes' in link['href'] or 'monetarypolicy/fomcstatement' in link['href']]\n",
    "    \n",
    "    # Filter links by start year\n",
    "    fomc_links = [link for link in fomc_links if any(str(year) in link for year in range(start_year, 2024))]\n",
    "    \n",
    "    # Fetch and store the content and dates\n",
    "    data = []\n",
    "    for link in fomc_links:\n",
    "        full_url = f'{base_url}{link}'\n",
    "        response = requests.get(full_url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        content = soup.get_text()\n",
    "        date = link.split('/')[-1].split('.')[0]  # Extract date from URL\n",
    "        data.append({'url': full_url, 'date': date, 'content': content})\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Fetch FOMC statements and minutes from 2000 onwards with dates\n",
    "fomc_data_with_dates = fetch_fomc_statements_with_dates(start_year=2000)\n",
    "\n",
    "# Display the first few rows of the data\n",
    "fomc_data_with_dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dac471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to preprocess text data\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove newline characters and extra spaces\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove any non-alphanumeric characters (except for basic punctuation)\n",
    "    text = re.sub(r'[^\\w\\s.,!?]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "# Apply preprocessing to the content column\n",
    "fomc_data_with_dates['cleaned_content'] = fomc_data_with_dates['content'].apply(preprocess_text)\n",
    "\n",
    "# Display the first few rows of the cleaned data\n",
    "fomc_data_with_dates[['date', 'cleaned_content']].head() 56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67453680",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "# Download stopwords if not already downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to preprocess text data further\n",
    "\n",
    "def preprocess_text_advanced(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    # Tokenize the text\n",
    "    words = word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # Rejoin words into a single string\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply advanced preprocessing to the cleaned content column\n",
    "fomc_data_with_dates['cleaned_content_advanced'] = fomc_data_with_dates['cleaned_content'].apply(preprocess_text_advanced)\n",
    "\n",
    "# Display the first few rows of the advanced cleaned data\n",
    "fomc_data_with_dates[['date', 'cleaned_content_advanced']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626994f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Function to convert date string to datetime object\n",
    "\n",
    "def convert_date(date_str):\n",
    "    # Extract year, month, and day from the string\n",
    "    year = int(date_str[-8:-4])\n",
    "    month = int(date_str[-4:-2])\n",
    "    day = int(date_str[-2:])\n",
    "    # Return a datetime object\n",
    "    return pd.to_datetime(f'{year}-{month}-{day}')\n",
    "\n",
    "# Apply the conversion to the date column\n",
    "fomc_data_with_dates['date_converted'] = fomc_data_with_dates['date'].apply(convert_date)\n",
    "\n",
    "# Display the first few rows with the converted date\n",
    "fomc_data_with_dates[['date', 'date_converted']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0fe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display an example of the cleaned text\n",
    "example_cleaned_text = fomc_data_with_dates['cleaned_content_advanced'].iloc[0]\n",
    "example_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d3780d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ef preprocess_text_further(text):\n",
    "    # Remove punctuation\n",
    "    text = re.sub(r'[^\b\ba-zA-Z\b\b]', ' ', text)\n",
    "    # Remove numbers\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply further preprocessing to the advanced cleaned content column\n",
    "fomc_data_with_dates['cleaned_content_final'] = fomc_data_with_dates['cleaned_content_advanced'].apply(preprocess_text_further)\n",
    "\n",
    "# Display an example of the further cleaned text\n",
    "example_further_cleaned_text = fomc_data_with_dates['cleaned_content_final'].iloc[0]\n",
    "example_further_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67030136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove specific unwanted words\n",
    "\n",
    "def remove_unwanted_words(text):\n",
    "    unwanted_words = ['www', 'htm', 'com']\n",
    "    for word in unwanted_words:\n",
    "        text = re.sub(rf'\\b{word}\\b', '', text)\n",
    "    # Remove extra spaces again\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Apply the removal of unwanted words to the final cleaned content column\n",
    "fomc_data_with_dates['cleaned_content_final'] = fomc_data_with_dates['cleaned_content_final'].apply(remove_unwanted_words)\n",
    "\n",
    "# Display an example of the further cleaned text\n",
    "example_further_cleaned_text = fomc_data_with_dates['cleaned_content_final'].iloc[0]\n",
    "example_further_cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbbaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vaderSentiment\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Initialize the VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Function to get sentiment scores\n",
    "\n",
    "def get_sentiment_scores(text):\n",
    "    return analyzer.polarity_scores(text)\n",
    "\n",
    "# Apply sentiment analysis to the cleaned text\n",
    "fomc_data_with_dates['sentiment_scores'] = fomc_data_with_dates['cleaned_content_final'].apply(get_sentiment_scores)\n",
    "\n",
    "# Display the sentiment scores for the first document\n",
    "fomc_data_with_dates['sentiment_scores'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6760f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the length of the first document's text\n",
    "len(fomc_data_with_dates['cleaned_content_final'].iloc[0])A"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
